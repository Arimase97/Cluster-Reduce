## Cluster-Reduce: Compressing Sketches for Distributed Data Streams

This repository contains all the related code of our paper "Cluster-Reduce: Compressing Sketches for Distributed Data Streams".

### Introduction

Sketches, a type of probabilistic algorithms, have been widely accepted as the approximate summary of data streams. Compressing sketches is the best choice in distributed data streams to reduce communication overhead. The ideal compression algorithm should meet the following three requirements: efficiency of compression procedure, support of direct query without decompression, and accuracy of compressed sketches. However, no prior work can meet these requirements at the same time. Especially, the accuracy is poor after compression. In this paper, we propose Cluster-Reduce, a framework for compressing sketches, which can meet all three requirements. Our key technique nearness clustering rearranges the adjacent counters with similar values in the sketch to significantly improve the accuracy. We use Cluster-Reduce to compress four kinds of sketches in two use-cases: distributed data streams and distributed machine learning. Extensive experimental results show that Cluster-Reduce can achieve up to 60 times smaller error than prior works. The source codes of Cluster-Reduce are available at Github anonymously.

### About This Repository

`experiment_<experiment name>.cpp` are source codes related to accuracy experiments.

Folder `throughput/` contains source codes related to the query throughput experiments.

### Requirements

1. Ubuntu 16.04 LTS
2. g++ >= 7.5.0

### Datasets

We conduct our experiment on two synthetic datasets following the Zipf distribution (Zipf_0.3 and Zipf_2.1, where 0.3 and 2.1 are the skewness), which are generated by Web Polygraph[5] and the following three real-world datasets.

```
IMC DC: http://pages.cs.wisc.edu/~tbenson/IMC10_Data.html
CAIDA: https://www.caida.org/data/overview/
Webpage: http://fimi.ua.ac.be/data/webdocs.dat.gz
```

### Usage

 Take `experiment_original_memory.cpp` as an example. Change the first parameter in function fileReader to the path to where you save the datasets. You can simply run the experiment program with the following command.

```
g++ experiment_original_memory.cpp -o experiment_original_memory
./experiment_original_memory
```



### Experiments on Distributed ML

First, download the dataset and put them into the root folder and rename it as `Twin_gas`.

You can simply build all the programs for this task with the following commands:

```
cd sketchML
make
```

You can also modify the parameters in `classification_params.h` and `regression_params.h`

After that, you can run `classification.out` or `regression.out` to execute the experiments.

### References

[1] Apache DataSketches. http://datasketches.apache.org/.

[2] Twin gas sensor arrays Data Set. http://archive.ics.uci.edu/ml/datasets/Twin+gas+sensor+arrays.
[3] The caida anonymized 2016 internet traces. http://www.caida.org/data/overview/.

[4] Real-life transactional dataset. http://fimi.ua.ac.be/data/.

[5] Alex Rousskov and Duane Wessels. High-performance benchmarking with web polygraph. Software: Practice and Experience, 34(2):187â€“211, 2004.